Attention mechanisms, which can help integrate more information into training GANs.

## RecurrentAttention1D

Attention mechanism for Recurrent Keras layers (VanillaRNN, GRU and LSTM) which pays attention to a tensor with shape `(batch_size, num_features)`.

## RecurrentAttention2D

Attention mechanism for Recurrent Keras layers (VanillaRNN, GRU and LSTM) which pays attention to a tensor with shape `(batch_size, num_timesteps, num_features)`.

